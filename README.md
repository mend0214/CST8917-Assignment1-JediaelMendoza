# CST8917 Assignment 1: Serverless Computing - Critical Analysis - Jediael Mendoza  - 041208322 - February 13, 2026

# Summary of “Serverless Computing: One Step Forward, Two Steps Back”

The central argument of the paper is that first-generation serverless platforms, especially Functions as a Service, represent meaningful progress in cloud computing because they provide true autoscaling and pay-as-you-go execution of code [1]. This is what the authors describe as “one step forward.” However, they argue these systems also take “two steps back” because they conflict with the needs of modern computing, which is increasingly data-centric and distributed [1]. In essence, serverless makes it easier to run small, event-driven tasks, but it makes it harder to build efficient large-scale systems that require fast data access, coordination, and long-lived components.

One limitation the authors emphasize is execution time constraints. Functions in first-generation serverless environments have fixed lifetimes and cannot reliably maintain state between invocations [1]. Even if a function sometimes reuses the same virtual machine through a warm start, there is no guarantee of continuity, so developers must assume that all in-memory state will be lost. This restriction makes it difficult to design long-running or stateful workflows without repeatedly saving and restoring state from external storage.

The paper also highlights serious communication and network limitations. Functions communicate with storage and services over shared network interfaces, creating I O bottlenecks. More importantly, function instances are not directly addressable like normal network endpoints, so they cannot easily communicate peer to peer [1]. Instead, coordination usually happens through intermediary storage services such as object stores or key-value databases, which are much slower and more expensive than direct messaging. This leads to higher latency and reduced efficiency in distributed or interactive systems.

A related issue is the “data shipping” anti-pattern. The authors argue that FaaS architectures often move large amounts of data to short-lived functions rather than moving computation closer to where the data already resides [1]. Because functions are isolated, ephemeral, and non-addressable, they cannot reliably cache or maintain local state. This design ignores memory hierarchy realities and results in unnecessary network transfers, increasing both cost and latency.

Another limitation is restricted access to specialized hardware. First-generation serverless platforms typically offer only basic CPU and memory allocations, with little or no access to GPUs or other accelerators [1]. The authors argue this is problematic because modern data analytics and machine learning increasingly depend on hardware specialization. Without access to such resources, serverless becomes a poor environment for performance-intensive or innovative data systems.

Finally, the paper discusses challenges for distributed computing and stateful workloads. Because functions cannot directly address each other, distributed protocols such as leader election or consistency management must pass through slow storage layers. This makes sophisticated distributed systems inefficient and costly, and it also slows workflows that need frequent state updates. Their case studies show that these architectural choices can lead to significant performance and cost penalties in realistic applications [1].

Rather than rejecting serverless entirely, the authors propose several future directions. They advocate for fluid code and data placement so systems can colocate computation with data when beneficial, shifting from data shipping toward code shipping [1]. They also call for heterogeneous hardware support that allows applications to leverage accelerators and hardware affinity [1]. In addition, they propose long-running, addressable virtual agents that behave like stable services while still supporting elasticity [1]. Other recommendations include new programming models better suited to distributed environments and improved support for service-level objectives. Together, these ideas aim to preserve autoscaling while enabling efficient, innovative cloud-scale systems.

# Azure Durable Functions Deep Dive

Azure Durable Functions extend basic serverless Functions-as-a-Service by adding built-in workflow coordination and state management. A client function starts and monitors orchestrations from external triggers like HTTP requests. An orchestrator function defines the workflow logic, schedules tasks, waits for results, and automatically saves progress so it can recover from failures or continue long-running processes. Activity functions perform the actual work such as API calls or data processing and remain stateless. Together, they allow developers to write sequential-style code while the framework handles retries, checkpoints, and parallel execution. Unlike basic FaaS, which is stateless and short-lived, Durable Functions provide long-running, stateful, and reliable workflows, effectively turning serverless computing into a managed workflow engine. Azure Durable Functions address many weaknesses identified in Hellerstein’s serverless critique, especially statelessness, poor coordination, and limited long-running workflows. Client, orchestrator, and activity functions introduce built-in state, retries, and workflow control, reducing manual storage management [2].

Durable Functions manage state by storing workflow progress in external storage instead of relying only on memory. They use three main ideas called event sourcing checkpointing and replay. Event sourcing means every action the orchestrator performs such as calling an activity or waiting for a timer is written to a persistent history log. Checkpointing automatically saves the workflow position whenever the code pauses so it can continue after crashes or scaling events. Replay rebuilds the workflow by running the orchestrator code again while reusing stored results instead of repeating tasks. According to Microsoft documentation this approach supports long running reliable workflows. It addresses the criticism that serverless functions are stateless by making the overall orchestration logically stateful without requiring developers to manage databases manually. The article criticizes basic serverless functions for being stateless and hard to coordinate. Durable Functions address this by automatically managing state through event histories checkpointing and replay, enabling long running workflows without manual databases. This fixes the workflow usability issues [3].

Orchestrator functions in Azure Durable Functions do not run forever. Instead, they avoid normal timeout limits by pausing execution and saving their state to durable storage whenever they reach an await point, such as waiting for an activity or timer. The runtime then unloads the function from memory. When the next event occurs, the system reloads the orchestrator and replays its history to restore variables and continue execution. Because the function repeatedly stops and resumes rather than running continuously, it is not treated as a single long execution. This design allows workflows to last hours or days. However, activity functions still behave like regular Azure Functions and remain limited by timeout, CPU, memory, and hosting plan constraints. The article argues that basic serverless functions fail at long running coordination because they are stateless and time limited. Durable orchestrators address this by pausing saving state and replaying instead of running continuously [3].

Orchestrator and activity functions communicate indirectly through the Durable Functions runtime and its storage system rather than through direct network calls or shared memory. When an orchestrator schedules an activity, it writes a message with the activity name and input data to durable storage. The runtime detects this message and executes the activity on an available worker. After completion, the activity writes its result back to storage as another event. The orchestrator is then reloaded and replays its execution history, reading the stored result to continue the workflow. This indirect communication model increases reliability and fault tolerance, but it is slower than real time in memory or direct network communication. The article explains that serverless functions communicate through storage instead of direct networking, which reduces performance. Durable Functions use the same storage based communication model, improving reliability and state management, but still introducing latency and overhead, reflecting the same architectural tradeoffs and coordination limitations discussed in the paper [4].

The fan-out/fan-in pattern lets an orchestrator run multiple tasks in parallel and then combine their results. In a typical scenario, the orchestrator first fans out by starting many activity functions concurrently to handle independent pieces of work. Each of these activities executes separately and can run on different workers. Once all the parallel tasks have been scheduled, the orchestrator then fans in by waiting for every activity to complete and aggregating their results into a final output. This pattern is useful for large workloads that benefit from concurrent execution and result consolidation. The article notes serverless handles parallel tasks well but struggles with coordination. Fan out and fan in improves parallel execution and aggregation, yet still relies on storage based communication that can add latency [5].

# Critical Evaluation
Azure Durable Functions significantly improve the usability of serverless computing, but they do not fully remove the deeper architectural limitations discussed in “Serverless Computing: One Step Forward, Two Steps Back.” Instead of fixing the root problems, they mainly add higher-level workflow and coordination features that make development easier while the underlying infrastructure constraints remain. This means Durable Functions enhance the programming experience but do not fundamentally change how serverless systems behave at a low level.

One limitation that remains only partially addressed is communication and networking efficiency. Durable Functions still depend on storage-based communication between orchestrator and activity functions. Although developers interact with simple patterns rather than raw storage calls, the system still writes and reads workflow events from durable storage behind the scenes. This approach improves reliability and fault tolerance because progress is always saved and can be restored after failures. However, it also introduces latency and performance overhead because communication is indirect instead of real-time. The original paper argues that serverless platforms struggle when functions cannot directly and efficiently message each other, and Durable Functions do not change this core reality. They simplify coordination logic but do not remove the performance costs that come from storage-mediated communication, especially in workloads that involve frequent interaction or large data volumes.

A second limitation that persists is the data shipping problem. Durable Functions manage workflow state more effectively than basic serverless functions, yet they still do not solve the issue of moving data to code instead of moving code closer to data. Activity functions usually retrieve data from external storage, process it, and then send results back to storage again. Meanwhile, the orchestrator is intentionally prevented from doing heavy data processing to keep workflows predictable and replayable. While this design improves stability and recovery, it reinforces the separation between computation and data location. As a result, large datasets still travel across networks, which can increase cost, delay, and inefficiency. The deeper concern raised in the research paper about poor data locality and unnecessary data movement therefore still applies.

Because of these unresolved challenges, Azure Durable Functions represent incremental rather than transformative progress. They align with the paper’s vision only in limited ways, such as enabling logically long-running workflows through checkpointing and replay. However, they do not significantly address direct networking between components, hardware specialization, or broader distributed efficiency concerns. Their primary strength is improving developer productivity and workflow reliability, not changing infrastructure performance characteristics.

My verdict overall is that Azure Durable Functions are best viewed as a practical workaround rather than a complete solution. They make serverless systems easier to build and maintain, especially for stateful or coordinated workflows. However, they do not eliminate the architectural tradeoffs emphasized in the paper, including indirect communication, network bottlenecks, and limited control over data placement. In short, Durable Functions make serverless easier to program, but they do not fundamentally resolve the performance and infrastructure constraints that must be overcome for true cloud innovation.

# References
[1] J. M. Hellerstein et al., “Serverless computing: one step forward, two steps back,” arXiv.org, Dec. 10, 2018. https://arxiv.org/abs/1812.03651

[2] “Function types in Azure Durable Functions,” Microsoft Learn. https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-types-features-overview

[3] “Durable orchestrations - azure functions,” Microsoft Learn. https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-orchestrations?tabs=csharp-inproc

[4] Syncfusion, “Durable Functions Fundamentals | Azure Durable Functions Succinctly® | Free Ebook | Syncfusion®,” Syncfusion. https://www.syncfusion.com/succinctly-free-ebooks/azure-durable-functions-succinctly/durable-functions-fundamentals

[5] “Fan-out/fan-in scenarios in Durable Functions - Azure,” Microsoft Learn. https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-cloud-backup?tabs=csharp

# AI Disclosure
- Used ChatGPT to help summarize *Serverless Computing: One Step Forward, Two Steps Back*
- Used ChatGPT to summarize the sources and connect them to the article
- Used ChatGPT for Part 3
- Used ChatGPT to improve grammar and wording
- Used ChatGPT to check references in IEEE format